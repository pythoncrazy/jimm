{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"jimm docs","text":"<p>These are the docs for the Jax Image Modeling of Models library. These models are implemented using flax nnx/jax with sharding annotations for each model.</p>"},{"location":"#models-implemented","title":"Models Implemented","text":"<ul> <li>Vision Transformers</li> <li>CLIP</li> <li>SigLIP</li> <li>more tbd</li> <li>contribute, it's open source!</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pixish","title":"Using pixi.sh:","text":"<p><code>pixi add timm@https://github.com/Locamage/jimm.git --pypi</code></p>"},{"location":"#using-uv","title":"Using uv","text":"<p><code>uv add --dev git+https://github.com/Locamage/jimm.git</code> or if you prefer to not add as a direct dependency: <code>uv pip install git+https://github.com/Locamage/jimm.git</code></p>"},{"location":"#using-pipconda","title":"Using pip/conda","text":"<p><code>pip install git+https://github.com/Locamage.git</code></p>"},{"location":"models/CLIP/","title":"CLIP (Contrastive Language\u2013Image Pre-training)","text":"<p>CLIP (Contrastive Language\u2013Image Pre-training) is a neural network architecture that learns visual concepts from natural language supervision. It is trained on a large dataset of image-text pairs to create a unified vision-language model that can understand both images and text in a shared semantic space.</p> <p>CLIP consists of two main components: 1. A vision encoder (Vision Transformer) that processes images into visual features 2. A text encoder (Transformer) that processes text into textual features</p> <p>The model is trained using contrastive learning, where it learns to maximize the cosine similarity between the embeddings of matching image-text pairs while minimizing it for non-matching pairs. This allows CLIP to perform zero-shot classification by comparing image embeddings with text embeddings of potential labels.</p> <p>CLIP was introduced in the paper \"Learning Transferable Visual Models From Natural Language Supervision\" and has shown remarkable zero-shot generalization capabilities across a wide range of visual classification tasks. The CLIP model combines a Vision Transformer and a Text Transformer to learn joint representations of images and text. It is trained to maximize the similarity between matching image-text pairs while minimizing similarity between non-matching pairs.</p>"},{"location":"models/CLIP/#jimm.models.clip.CLIP","title":"<code>jimm.models.clip.CLIP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>class CLIP(nnx.Module):\n    def __init__(\n        self,\n        image_resolution: int,\n        vision_layers: int,\n        vision_width: int,\n        vision_patch_size: int,\n        context_length: int,\n        vocab_size: int,\n        transformer_width: int,\n        transformer_heads: int,\n        transformer_layers: int,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        mesh: Mesh | None = None,\n    ):\n        \"\"\"\n        Initialize the CLIP model.\n\n        Args:\n            image_resolution (int): The resolution of the input images.\n            vision_layers (int): The number of layers in the vision transformer.\n            vision_width (int): The width of the vision transformer.\n            vision_patch_size (int): The patch size of the vision transformer.\n            context_length (int): The length of the context.\n            vocab_size (int): The size of the vocabulary.\n            transformer_width (int): The width of the transformer.\n            transformer_heads (int): The number of attention heads in the transformer.\n            transformer_layers (int): The number of layers in the transformer.\n            rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n            dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n            mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n        \"\"\"\n        self.vision_layers = vision_layers\n        self.vision_width = vision_width\n        self.vision_patch_size = vision_patch_size\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.transformer_width = transformer_width\n        self.transformer_heads = transformer_heads\n        self.transformer_layers = transformer_layers\n        self.dtype = dtype\n\n        vision_heads = vision_width // 64\n\n        self.attn_mask: Float[Array, \"context_length context_length\"] = jnp.tril(jnp.ones((context_length, context_length), dtype=dtype))\n\n        self.vision_model = VisionTransformerBase(\n            img_size=image_resolution,\n            patch_size=vision_patch_size,\n            in_channels=3,\n            hidden_size=vision_width,\n            num_layers=vision_layers,\n            num_heads=vision_heads,\n            mlp_dim=vision_width * 4,\n            use_pre_norm=True,\n            use_patch_bias=False,\n            use_quick_gelu=True,\n            pooling_type=\"CLS\",\n            layernorm_epsilon=1e-5,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n        self.visual_projection = nnx.Linear(\n            vision_width,\n            transformer_width,\n            use_bias=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n        )\n\n        self.text_model = Transformer(\n            width=transformer_width,\n            mlp_dim=transformer_width * 4,\n            layers=transformer_layers,\n            num_heads=transformer_heads,\n            dropout_rate=0.0,\n            attn_mask=self.attn_mask,\n            use_quick_gelu=True,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n        self.vocab_size = vocab_size\n        self.token_embedding = nnx.Embed(\n            num_embeddings=vocab_size,\n            features=transformer_width,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n        self.ln_final = nnx.LayerNorm(\n            transformer_width,\n            epsilon=1e-5,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        self.text_projection = nnx.Linear(\n            transformer_width,\n            transformer_width,\n            use_bias=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n\n    def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode images into embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Image embeddings.\n        \"\"\"\n        features = self.vision_model(image)\n        return self.visual_projection(features)\n\n    def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode text tokens into embeddings.\n\n        Args:\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Text embeddings.\n        \"\"\"\n        seq_len = text.shape[1]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n        x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding.value[:seq_len]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n        x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n        eot_token_pos: Float[Array, \" batch \"] = jnp.argmax(text, axis=-1)\n        batch_indices: Float[Array, \" batch \"] = jnp.arange(x.shape[0])\n        x: Float[Array, \"batch transformer_width\"] = x[batch_indices, eot_token_pos] @ self.text_projection.kernel.value\n        return x\n\n    def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n        \"\"\"\n        Calculate similarity between image and text embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n        \"\"\"\n        image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n        text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n        image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n        text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n        logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n        logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T\n        return logits\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"CLIP\":\n        \"\"\"Load a pretrained CLIP model from a local path or HuggingFace Hub.\n\n        Args:\n            model_name_or_path (str): Path to local weights or HuggingFace model ID.\n            use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n            mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n        Returns:\n            CLIP: Pretrained CLIP model\n        \"\"\"\n\n        params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n        config: dict[str, Any] = config_dict\n\n        if config == {}:\n            if not use_pytorch:\n                text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n                text_max_pos_embed = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n                text_vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n\n                text_num_layers = 0\n                for k_param in params_fstate:\n                    if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                        layer_idx = int(k_param.split(\".\")[3])\n                        text_num_layers = max(text_num_layers, layer_idx + 1)\n\n                vision_hidden_size = params_fstate[\"vision_model.embeddings.class_embedding\"].shape[0]\n                vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[2]\n                vision_image_size = int((params_fstate[\"vision_model.embeddings.position_embedding.weight\"].shape[0] - 1) ** 0.5) * vision_patch_size\n\n                vision_num_layers = 0\n                for k_param in params_fstate:\n                    if k_param.startswith(\"vision_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                        layer_idx = int(k_param.split(\".\")[3])\n                        vision_num_layers = max(vision_num_layers, layer_idx + 1)\n\n                config = {\n                    \"text_config\": {\n                        \"hidden_size\": text_hidden_size,\n                        \"num_attention_heads\": text_hidden_size // 64,\n                        \"num_hidden_layers\": text_num_layers,\n                        \"max_position_embeddings\": text_max_pos_embed,\n                        \"vocab_size\": text_vocab_size,\n                    },\n                    \"vision_config\": {\n                        \"hidden_size\": vision_hidden_size,\n                        \"num_attention_heads\": vision_hidden_size // 64,\n                        \"num_hidden_layers\": vision_num_layers,\n                        \"image_size\": vision_image_size,\n                        \"patch_size\": vision_patch_size,\n                    },\n                }\n            else:\n                raise ValueError(f\"Configuration could not be loaded for PyTorch model {model_name_or_path}\")\n\n        text_config = config[\"text_config\"]\n        vision_config = config[\"vision_config\"]\n\n        model = cls(\n            image_resolution=vision_config[\"image_size\"],\n            vision_layers=vision_config[\"num_hidden_layers\"],\n            vision_width=vision_config[\"hidden_size\"],\n            vision_patch_size=vision_config[\"patch_size\"],\n            context_length=text_config[\"max_position_embeddings\"],\n            vocab_size=text_config[\"vocab_size\"],\n            transformer_width=text_config[\"hidden_size\"],\n            transformer_heads=text_config[\"num_attention_heads\"],\n            transformer_layers=text_config[\"num_hidden_layers\"],\n            mesh=mesh,\n            dtype=dtype,\n            param_dtype=dtype,\n        )\n\n        flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n        mapping_list = [\n            ((\"logit_scale\",), (\"logit_scale\",)),\n            ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n            ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n            ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n            ((\"text_projection\", \"kernel\"), (\"text_projection\", \"weight\")),\n            ((\"vision_model\", \"cls_token\"), (\"vision_model\", \"embeddings\", \"class_embedding\")),\n            ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"vision_model\", \"patch_embeddings\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n            ((\"vision_model\", \"ln_pre\", \"scale\"), (\"vision_model\", \"pre_layrnorm\", \"weight\")),\n            ((\"vision_model\", \"ln_pre\", \"bias\"), (\"vision_model\", \"pre_layrnorm\", \"bias\")),\n            ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n            ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n            ((\"visual_projection\", \"kernel\"), (\"visual_projection\", \"weight\")),\n        ]\n\n        for i in range(text_config[\"num_hidden_layers\"]):\n            flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n            hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        for i in range(vision_config[\"num_hidden_layers\"]):\n            flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n            hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        params_name_mapping = dict(mapping_list)\n        nonvisited = set(flax_model_params_fstate.keys())\n\n        hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n        used_hf_keys: Set[str] = set()\n\n        for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n            if flax_dst_key_tuple not in flax_model_params_fstate:\n                continue\n\n            hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n            if hf_src_key_as_string not in params_fstate:\n                continue\n\n            used_hf_keys.add(hf_src_key_as_string)\n            nonvisited.discard(flax_dst_key_tuple)\n            src_value = params_fstate[hf_src_key_as_string]\n            dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n            original_param_sharding = dst_value_obj.value.sharding\n\n            if flax_dst_key_tuple == (\"vision_model\", \"patch_embeddings\", \"kernel\"):\n                src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n            elif flax_dst_key_tuple == (\"vision_model\", \"cls_token\"):\n                src_value = src_value.reshape(1, 1, -1)\n            elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n                src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n            elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((hidden_size, num_heads, head_dim))\n            elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((num_heads, head_dim))\n            elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((num_heads, head_dim, hidden_size))\n            elif flax_dst_key_tuple == (\"token_embedding\", \"embedding\"):\n                pass\n            elif flax_dst_key_tuple == (\"positional_embedding\",):\n                pass\n            elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n                src_value = jnp.transpose(src_value, (1, 0))\n\n            if src_value.shape != dst_value_obj.value.shape:\n                raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n            sharded_new_value = jax.device_put(src_value, original_param_sharding)\n            dst_value_obj.value = sharded_new_value\n\n        nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n        assert len(nonvisited) == 0, f\"Some Flax CLIP model parameters were not visited: {sorted(list(nonvisited))}\"\n\n        leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n        known_unused_hf_buffer_keys = {\n            \"text_model.embeddings.position_ids\",\n            \"vision_model.embeddings.position_ids\",\n        }\n        unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n        assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n        return model\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.__call__","title":"<code>__call__(image, text)</code>","text":"<p>Calculate similarity between image and text embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch batch']</code> <p>Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n    \"\"\"\n    Calculate similarity between image and text embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n    \"\"\"\n    image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n    text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n    image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n    text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n    logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n    logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T\n    return logits\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.__init__","title":"<code>__init__(image_resolution, vision_layers, vision_width, vision_patch_size, context_length, vocab_size, transformer_width, transformer_heads, transformer_layers, rngs=nnx.Rngs(0), dtype=jnp.float32, param_dtype=jnp.float32, mesh=None)</code>","text":"<p>Initialize the CLIP model.</p> <p>Parameters:</p> Name Type Description Default <code>image_resolution</code> <code>int</code> <p>The resolution of the input images.</p> required <code>vision_layers</code> <code>int</code> <p>The number of layers in the vision transformer.</p> required <code>vision_width</code> <code>int</code> <p>The width of the vision transformer.</p> required <code>vision_patch_size</code> <code>int</code> <p>The patch size of the vision transformer.</p> required <code>context_length</code> <code>int</code> <p>The length of the context.</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary.</p> required <code>transformer_width</code> <code>int</code> <p>The width of the transformer.</p> required <code>transformer_heads</code> <code>int</code> <p>The number of attention heads in the transformer.</p> required <code>transformer_layers</code> <code>int</code> <p>The number of layers in the transformer.</p> required <code>rngs</code> <code>Rngs</code> <p>The random number generator state. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>The data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>mesh</code> <code>Mesh | None</code> <p>The device mesh for parameter sharding. Defaults to None.</p> <code>None</code> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __init__(\n    self,\n    image_resolution: int,\n    vision_layers: int,\n    vision_width: int,\n    vision_patch_size: int,\n    context_length: int,\n    vocab_size: int,\n    transformer_width: int,\n    transformer_heads: int,\n    transformer_layers: int,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    mesh: Mesh | None = None,\n):\n    \"\"\"\n    Initialize the CLIP model.\n\n    Args:\n        image_resolution (int): The resolution of the input images.\n        vision_layers (int): The number of layers in the vision transformer.\n        vision_width (int): The width of the vision transformer.\n        vision_patch_size (int): The patch size of the vision transformer.\n        context_length (int): The length of the context.\n        vocab_size (int): The size of the vocabulary.\n        transformer_width (int): The width of the transformer.\n        transformer_heads (int): The number of attention heads in the transformer.\n        transformer_layers (int): The number of layers in the transformer.\n        rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n        dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n        mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n    \"\"\"\n    self.vision_layers = vision_layers\n    self.vision_width = vision_width\n    self.vision_patch_size = vision_patch_size\n    self.context_length = context_length\n    self.vocab_size = vocab_size\n    self.transformer_width = transformer_width\n    self.transformer_heads = transformer_heads\n    self.transformer_layers = transformer_layers\n    self.dtype = dtype\n\n    vision_heads = vision_width // 64\n\n    self.attn_mask: Float[Array, \"context_length context_length\"] = jnp.tril(jnp.ones((context_length, context_length), dtype=dtype))\n\n    self.vision_model = VisionTransformerBase(\n        img_size=image_resolution,\n        patch_size=vision_patch_size,\n        in_channels=3,\n        hidden_size=vision_width,\n        num_layers=vision_layers,\n        num_heads=vision_heads,\n        mlp_dim=vision_width * 4,\n        use_pre_norm=True,\n        use_patch_bias=False,\n        use_quick_gelu=True,\n        pooling_type=\"CLS\",\n        layernorm_epsilon=1e-5,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n    self.visual_projection = nnx.Linear(\n        vision_width,\n        transformer_width,\n        use_bias=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n    )\n\n    self.text_model = Transformer(\n        width=transformer_width,\n        mlp_dim=transformer_width * 4,\n        layers=transformer_layers,\n        num_heads=transformer_heads,\n        dropout_rate=0.0,\n        attn_mask=self.attn_mask,\n        use_quick_gelu=True,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n    self.vocab_size = vocab_size\n    self.token_embedding = nnx.Embed(\n        num_embeddings=vocab_size,\n        features=transformer_width,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n    self.ln_final = nnx.LayerNorm(\n        transformer_width,\n        epsilon=1e-5,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    self.text_projection = nnx.Linear(\n        transformer_width,\n        transformer_width,\n        use_bias=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.encode_image","title":"<code>encode_image(image)</code>","text":"<p>Encode images into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Image embeddings.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode images into embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Image embeddings.\n    \"\"\"\n    features = self.vision_model(image)\n    return self.visual_projection(features)\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.encode_text","title":"<code>encode_text(text)</code>","text":"<p>Encode text tokens into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Text embeddings.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode text tokens into embeddings.\n\n    Args:\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Text embeddings.\n    \"\"\"\n    seq_len = text.shape[1]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n    x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding.value[:seq_len]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n    x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n    eot_token_pos: Float[Array, \" batch \"] = jnp.argmax(text, axis=-1)\n    batch_indices: Float[Array, \" batch \"] = jnp.arange(x.shape[0])\n    x: Float[Array, \"batch transformer_width\"] = x[batch_indices, eot_token_pos] @ self.text_projection.kernel.value\n    return x\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.from_pretrained","title":"<code>from_pretrained(model_name_or_path, use_pytorch=False, mesh=None, dtype=jnp.float32)</code>  <code>classmethod</code>","text":"<p>Load a pretrained CLIP model from a local path or HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path to local weights or HuggingFace model ID.</p> required <code>use_pytorch</code> <code>bool</code> <p>Whether to load from PyTorch weights. Defaults to False.</p> <code>False</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional device mesh for parameter sharding. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>CLIP</code> <code>CLIP</code> <p>Pretrained CLIP model</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"CLIP\":\n    \"\"\"Load a pretrained CLIP model from a local path or HuggingFace Hub.\n\n    Args:\n        model_name_or_path (str): Path to local weights or HuggingFace model ID.\n        use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n        mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n    Returns:\n        CLIP: Pretrained CLIP model\n    \"\"\"\n\n    params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n    config: dict[str, Any] = config_dict\n\n    if config == {}:\n        if not use_pytorch:\n            text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n            text_max_pos_embed = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n            text_vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n\n            text_num_layers = 0\n            for k_param in params_fstate:\n                if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                    layer_idx = int(k_param.split(\".\")[3])\n                    text_num_layers = max(text_num_layers, layer_idx + 1)\n\n            vision_hidden_size = params_fstate[\"vision_model.embeddings.class_embedding\"].shape[0]\n            vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[2]\n            vision_image_size = int((params_fstate[\"vision_model.embeddings.position_embedding.weight\"].shape[0] - 1) ** 0.5) * vision_patch_size\n\n            vision_num_layers = 0\n            for k_param in params_fstate:\n                if k_param.startswith(\"vision_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                    layer_idx = int(k_param.split(\".\")[3])\n                    vision_num_layers = max(vision_num_layers, layer_idx + 1)\n\n            config = {\n                \"text_config\": {\n                    \"hidden_size\": text_hidden_size,\n                    \"num_attention_heads\": text_hidden_size // 64,\n                    \"num_hidden_layers\": text_num_layers,\n                    \"max_position_embeddings\": text_max_pos_embed,\n                    \"vocab_size\": text_vocab_size,\n                },\n                \"vision_config\": {\n                    \"hidden_size\": vision_hidden_size,\n                    \"num_attention_heads\": vision_hidden_size // 64,\n                    \"num_hidden_layers\": vision_num_layers,\n                    \"image_size\": vision_image_size,\n                    \"patch_size\": vision_patch_size,\n                },\n            }\n        else:\n            raise ValueError(f\"Configuration could not be loaded for PyTorch model {model_name_or_path}\")\n\n    text_config = config[\"text_config\"]\n    vision_config = config[\"vision_config\"]\n\n    model = cls(\n        image_resolution=vision_config[\"image_size\"],\n        vision_layers=vision_config[\"num_hidden_layers\"],\n        vision_width=vision_config[\"hidden_size\"],\n        vision_patch_size=vision_config[\"patch_size\"],\n        context_length=text_config[\"max_position_embeddings\"],\n        vocab_size=text_config[\"vocab_size\"],\n        transformer_width=text_config[\"hidden_size\"],\n        transformer_heads=text_config[\"num_attention_heads\"],\n        transformer_layers=text_config[\"num_hidden_layers\"],\n        mesh=mesh,\n        dtype=dtype,\n        param_dtype=dtype,\n    )\n\n    flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n    mapping_list = [\n        ((\"logit_scale\",), (\"logit_scale\",)),\n        ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n        ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n        ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n        ((\"text_projection\", \"kernel\"), (\"text_projection\", \"weight\")),\n        ((\"vision_model\", \"cls_token\"), (\"vision_model\", \"embeddings\", \"class_embedding\")),\n        ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"vision_model\", \"patch_embeddings\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n        ((\"vision_model\", \"ln_pre\", \"scale\"), (\"vision_model\", \"pre_layrnorm\", \"weight\")),\n        ((\"vision_model\", \"ln_pre\", \"bias\"), (\"vision_model\", \"pre_layrnorm\", \"bias\")),\n        ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n        ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n        ((\"visual_projection\", \"kernel\"), (\"visual_projection\", \"weight\")),\n    ]\n\n    for i in range(text_config[\"num_hidden_layers\"]):\n        flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n        hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    for i in range(vision_config[\"num_hidden_layers\"]):\n        flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n        hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    params_name_mapping = dict(mapping_list)\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n    used_hf_keys: Set[str] = set()\n\n    for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n        if flax_dst_key_tuple not in flax_model_params_fstate:\n            continue\n\n        hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n        if hf_src_key_as_string not in params_fstate:\n            continue\n\n        used_hf_keys.add(hf_src_key_as_string)\n        nonvisited.discard(flax_dst_key_tuple)\n        src_value = params_fstate[hf_src_key_as_string]\n        dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n        original_param_sharding = dst_value_obj.value.sharding\n\n        if flax_dst_key_tuple == (\"vision_model\", \"patch_embeddings\", \"kernel\"):\n            src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n        elif flax_dst_key_tuple == (\"vision_model\", \"cls_token\"):\n            src_value = src_value.reshape(1, 1, -1)\n        elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n            src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n        elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((hidden_size, num_heads, head_dim))\n        elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((num_heads, head_dim))\n        elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((num_heads, head_dim, hidden_size))\n        elif flax_dst_key_tuple == (\"token_embedding\", \"embedding\"):\n            pass\n        elif flax_dst_key_tuple == (\"positional_embedding\",):\n            pass\n        elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n            src_value = jnp.transpose(src_value, (1, 0))\n\n        if src_value.shape != dst_value_obj.value.shape:\n            raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n        sharded_new_value = jax.device_put(src_value, original_param_sharding)\n        dst_value_obj.value = sharded_new_value\n\n    nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n    assert len(nonvisited) == 0, f\"Some Flax CLIP model parameters were not visited: {sorted(list(nonvisited))}\"\n\n    leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n    known_unused_hf_buffer_keys = {\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    }\n    unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n    assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n    return model\n</code></pre>"},{"location":"models/SigLIP/","title":"SigLIP (Sigmoid Loss for Language Image Pre-Training)","text":"<p>SigLIP (Sigmoid Loss for Language Image Pre-Training) is a vision-language model that builds upon the principles of CLIP but introduces a key architectural change: it uses a sigmoid loss function instead of the softmax-based contrastive loss. Additionally, there are some slight implementation differences (no attention_mask for the text encoder, padding the text inputs, multihead attention pooling for the vision encoder rather than a linear projection layer).</p> <p>This modification simplifies the training objective by treating the problem as a binary classification for each image-text pair (i.e., are they a positive or negative match?). This approach avoids the need for a global normalization over all pairs in a batch, which makes it more scalable and robust to noisy, web-scale data.</p> <p>Key features of SigLIP: 1.  Vision Encoder: A Vision Transformer (ViT) with a Multi-Head Attention Pooling (MAP) head. 2.  Text Encoder: A standard Transformer model. 3.  Sigmoid Loss: Enables training on larger batches and noisier datasets without requiring careful data curation or complex negative sampling strategies.</p> <p>SigLIP was introduced in the paper \"Sigmoid Loss for Language Image Pre-Training\" and has demonstrated improved performance and training efficiency.</p>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP","title":"<code>jimm.models.siglip.SigLIP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>class SigLIP(nnx.Module):\n    def __init__(\n        self,\n        image_resolution: int,\n        vision_layers: int,\n        vision_width: int,\n        vision_patch_size: int,\n        context_length: int,\n        vocab_size: int,\n        transformer_width: int,\n        transformer_heads: int,\n        transformer_layers: int,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        mesh: Mesh | None = None,\n    ):\n        \"\"\"\n        Initialize the SigLIP model.\n\n        Args:\n            image_resolution (int): The resolution of the input images.\n            vision_layers (int): The number of layers in the vision transformer.\n            vision_width (int): The width of the vision transformer.\n            vision_patch_size (int): The patch size of the vision transformer.\n            context_length (int): The length of the context.\n            vocab_size (int): The size of the vocabulary.\n            transformer_width (int): The width of the transformer.\n            transformer_heads (int): The number of attention heads in the transformer.\n            transformer_layers (int): The number of layers in the transformer.\n            rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n            dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n            mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n        \"\"\"\n        self.vision_layers = vision_layers\n        self.vision_width = vision_width\n        self.vision_patch_size = vision_patch_size\n        self.context_length = context_length\n        self.transformer_width = transformer_width\n        self.transformer_heads = transformer_heads\n        self.transformer_layers = transformer_layers\n        self.dtype = dtype\n\n        self.vision_heads = vision_width // 64\n        self.vision_model = VisionTransformerBase(\n            img_size=image_resolution,\n            patch_size=vision_patch_size,\n            in_channels=3,\n            hidden_size=vision_width,\n            num_layers=vision_layers,\n            num_heads=self.vision_heads,\n            mlp_dim=vision_width * 4,\n            use_pre_norm=False,\n            use_patch_bias=True,\n            use_quick_gelu=False,\n            pooling_type=\"MAP\",\n            layernorm_epsilon=1e-6,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n\n        self.text_model = Transformer(\n            width=transformer_width,\n            mlp_dim=transformer_width * 4,\n            layers=transformer_layers,\n            num_heads=transformer_heads,\n            dropout_rate=0.0,\n            layernorm_epsilon=1e-6,\n            use_quick_gelu=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n        self.vocab_size = vocab_size\n        self.token_embedding = nnx.Embed(\n            num_embeddings=vocab_size,\n            features=transformer_width,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n        self.ln_final = nnx.LayerNorm(\n            transformer_width,\n            epsilon=1e-6,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        self.text_projection = nnx.Linear(\n            transformer_width,\n            transformer_width,\n            use_bias=True,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n        self.logit_bias = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n\n    def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode images into embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Image embeddings.\n        \"\"\"\n        return self.vision_model(image)\n\n    def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode text tokens into embeddings.\n\n        Args:\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Text embeddings.\n        \"\"\"\n        seq_len = text.shape[1]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n        x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding[:seq_len]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n        x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n        pooled_output = x[:, -1, :]\n        x: Float[Array, \"batch transformer_width\"] = self.text_projection(pooled_output)\n        return x\n\n    def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n        \"\"\"\n        Calculate similarity between image and text embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n        \"\"\"\n        image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n        text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n        image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n        text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n        logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n        logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T + self.logit_bias.value\n        return logits\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"SigLIP\":\n        \"\"\"Load a pretrained SigLIP model from a local path or HuggingFace Hub.\n\n        Args:\n            model_name_or_path (str): Path to local weights or HuggingFace model ID.\n            use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n            mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n        Returns:\n            SigLIP: Pretrained SigLIP model\n        \"\"\"\n        params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n        config: dict[str, Any] = config_dict\n\n        vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[3]\n        vision_width = params_fstate[\"vision_model.embeddings.patch_embedding.bias\"].shape[0]\n        vision_num_layers = 0\n        for k in params_fstate:\n            if k.startswith(\"vision_model.encoder.layers.\") and k.endswith(\".mlp.fc2.bias\"):\n                vision_num_layers = max(vision_num_layers, int(k.split(\".\")[3]) + 1)\n\n        context_length = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n        vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n        text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n        text_num_layers = 0\n        for k_param in params_fstate:\n            if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                layer_idx = int(k_param.split(\".\")[3])\n                text_num_layers = max(text_num_layers, layer_idx + 1)\n\n        model = cls(\n            image_resolution=config[\"vision_config\"][\"image_size\"],\n            vision_layers=vision_num_layers,\n            vision_width=vision_width,\n            vision_patch_size=vision_patch_size,\n            context_length=context_length,\n            vocab_size=vocab_size,\n            transformer_width=text_hidden_size,\n            transformer_heads=text_hidden_size // 64,\n            transformer_layers=text_num_layers,\n            mesh=mesh,\n            dtype=dtype,\n            param_dtype=dtype,\n        )\n\n        flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n        nonvisited = set(flax_model_params_fstate.keys())\n        used_hf_keys: Set[str] = set()\n\n        mapping_list = [\n            ((\"logit_scale\",), (\"logit_scale\",)),\n            ((\"logit_bias\",), (\"logit_bias\",)),\n            ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n            ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n            ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n            ((\"text_projection\", \"kernel\"), (\"text_model\", \"head\", \"weight\")),\n            ((\"text_projection\", \"bias\"), (\"text_model\", \"head\", \"bias\")),\n            ((\"vision_model\", \"patch_embeddings\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n            ((\"vision_model\", \"patch_embeddings\", \"bias\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"bias\")),\n            ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n            ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n            ((\"vision_model\", \"MAPHead\", \"probe\"), (\"vision_model\", \"head\", \"probe\")),\n            ((\"vision_model\", \"MAPHead\", \"layernorm\", \"scale\"), (\"vision_model\", \"head\", \"layernorm\", \"weight\")),\n            ((\"vision_model\", \"MAPHead\", \"layernorm\", \"bias\"), (\"vision_model\", \"head\", \"layernorm\", \"bias\")),\n            ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 0, \"kernel\"), (\"vision_model\", \"head\", \"mlp\", \"fc1\", \"weight\")),\n            ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 0, \"bias\"), (\"vision_model\", \"head\", \"mlp\", \"fc1\", \"bias\")),\n            ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 2, \"kernel\"), (\"vision_model\", \"head\", \"mlp\", \"fc2\", \"weight\")),\n            ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 2, \"bias\"), (\"vision_model\", \"head\", \"mlp\", \"fc2\", \"bias\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"query\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"query\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"key\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"key\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"value\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"value\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"out\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"out_proj\", \"weight\")),\n            ((\"vision_model\", \"MAPHead\", \"attn\", \"out\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"out_proj\", \"bias\")),\n        ]\n\n        for i in range(text_num_layers):\n            flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n            hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        for i in range(vision_num_layers):\n            flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n            hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        params_name_mapping = dict(mapping_list)\n\n        for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n            hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n            nonvisited.discard(flax_dst_key_tuple)\n            used_hf_keys.add(hf_src_key_as_string)\n            src_value = params_fstate[hf_src_key_as_string]\n            dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n            original_param_sharding = dst_value_obj.value.sharding\n\n            if flax_dst_key_tuple == (\"vision_model\", \"patch_embeddings\", \"kernel\"):\n                src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n            elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n                src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n            elif flax_dst_key_tuple in [(\"logit_scale\",), (\"logit_bias\",)]:\n                src_value = jnp.squeeze(src_value)\n            elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if \"text_model\" in hf_src_key_as_string:\n                    num_heads = model.transformer_heads\n                    head_dim = model.transformer_width // num_heads\n                    src_value = src_value.reshape((model.transformer_width, num_heads, head_dim))\n                else:\n                    num_heads = model.vision_heads\n                    head_dim = vision_width // num_heads\n                    src_value = src_value.reshape((vision_width, num_heads, head_dim))\n            elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                if \"text_model\" in hf_src_key_as_string:\n                    num_heads = model.transformer_heads\n                    head_dim = model.transformer_width // num_heads\n                else:\n                    num_heads = model.vision_heads\n                    head_dim = vision_width // num_heads\n                src_value = src_value.reshape((num_heads, head_dim))\n            elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if \"text_model\" in hf_src_key_as_string:\n                    num_heads = model.transformer_heads\n                    head_dim = model.transformer_width // num_heads\n                    src_value = src_value.reshape((num_heads, head_dim, model.transformer_width))\n                else:\n                    num_heads = model.vision_heads\n                    head_dim = vision_width // num_heads\n                    src_value = src_value.reshape((num_heads, head_dim, vision_width))\n            elif hf_src_key_tuple[-1] == \"in_proj_weight\":\n                num_heads = model.vision_heads\n                head_dim = vision_width // num_heads\n                q_w, k_w, v_w = jnp.split(src_value, 3, axis=0)\n                w_map = {\"query\": q_w, \"key\": k_w, \"value\": v_w}\n                src_value = jnp.transpose(w_map[flax_dst_key_tuple[-2]], (1, 0)).reshape(vision_width, num_heads, head_dim)\n            elif hf_src_key_tuple[-1] == \"in_proj_bias\":\n                num_heads = model.vision_heads\n                head_dim = vision_width // num_heads\n                q_b, k_b, v_b = jnp.split(src_value, 3, axis=0)\n                b_map = {\"query\": q_b, \"key\": k_b, \"value\": v_b}\n                src_value = b_map[flax_dst_key_tuple[-2]].reshape(num_heads, head_dim)\n            elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n                if \"position_embedding\" not in hf_src_key_as_string and \"token_embedding\" not in hf_src_key_as_string:\n                    src_value = jnp.transpose(src_value, (1, 0))\n            if src_value.shape != dst_value_obj.value.shape:\n                raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n            sharded_new_value = jax.device_put(src_value, original_param_sharding)\n            dst_value_obj.value = sharded_new_value\n\n        nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n        hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n        leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n        known_unused_hf_buffer_keys = {\n            \"text_model.embeddings.position_ids\",\n            \"vision_model.embeddings.position_ids\",\n        }\n        unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n        assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n        return model\n</code></pre>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP.__call__","title":"<code>__call__(image, text)</code>","text":"<p>Calculate similarity between image and text embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch batch']</code> <p>Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.</p> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n    \"\"\"\n    Calculate similarity between image and text embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n    \"\"\"\n    image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n    text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n    image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n    text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n    logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n    logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T + self.logit_bias.value\n    return logits\n</code></pre>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP.__init__","title":"<code>__init__(image_resolution, vision_layers, vision_width, vision_patch_size, context_length, vocab_size, transformer_width, transformer_heads, transformer_layers, rngs=nnx.Rngs(0), dtype=jnp.float32, param_dtype=jnp.float32, mesh=None)</code>","text":"<p>Initialize the SigLIP model.</p> <p>Parameters:</p> Name Type Description Default <code>image_resolution</code> <code>int</code> <p>The resolution of the input images.</p> required <code>vision_layers</code> <code>int</code> <p>The number of layers in the vision transformer.</p> required <code>vision_width</code> <code>int</code> <p>The width of the vision transformer.</p> required <code>vision_patch_size</code> <code>int</code> <p>The patch size of the vision transformer.</p> required <code>context_length</code> <code>int</code> <p>The length of the context.</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary.</p> required <code>transformer_width</code> <code>int</code> <p>The width of the transformer.</p> required <code>transformer_heads</code> <code>int</code> <p>The number of attention heads in the transformer.</p> required <code>transformer_layers</code> <code>int</code> <p>The number of layers in the transformer.</p> required <code>rngs</code> <code>Rngs</code> <p>The random number generator state. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>The data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>mesh</code> <code>Mesh | None</code> <p>The device mesh for parameter sharding. Defaults to None.</p> <code>None</code> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>def __init__(\n    self,\n    image_resolution: int,\n    vision_layers: int,\n    vision_width: int,\n    vision_patch_size: int,\n    context_length: int,\n    vocab_size: int,\n    transformer_width: int,\n    transformer_heads: int,\n    transformer_layers: int,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    mesh: Mesh | None = None,\n):\n    \"\"\"\n    Initialize the SigLIP model.\n\n    Args:\n        image_resolution (int): The resolution of the input images.\n        vision_layers (int): The number of layers in the vision transformer.\n        vision_width (int): The width of the vision transformer.\n        vision_patch_size (int): The patch size of the vision transformer.\n        context_length (int): The length of the context.\n        vocab_size (int): The size of the vocabulary.\n        transformer_width (int): The width of the transformer.\n        transformer_heads (int): The number of attention heads in the transformer.\n        transformer_layers (int): The number of layers in the transformer.\n        rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n        dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n        mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n    \"\"\"\n    self.vision_layers = vision_layers\n    self.vision_width = vision_width\n    self.vision_patch_size = vision_patch_size\n    self.context_length = context_length\n    self.transformer_width = transformer_width\n    self.transformer_heads = transformer_heads\n    self.transformer_layers = transformer_layers\n    self.dtype = dtype\n\n    self.vision_heads = vision_width // 64\n    self.vision_model = VisionTransformerBase(\n        img_size=image_resolution,\n        patch_size=vision_patch_size,\n        in_channels=3,\n        hidden_size=vision_width,\n        num_layers=vision_layers,\n        num_heads=self.vision_heads,\n        mlp_dim=vision_width * 4,\n        use_pre_norm=False,\n        use_patch_bias=True,\n        use_quick_gelu=False,\n        pooling_type=\"MAP\",\n        layernorm_epsilon=1e-6,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n\n    self.text_model = Transformer(\n        width=transformer_width,\n        mlp_dim=transformer_width * 4,\n        layers=transformer_layers,\n        num_heads=transformer_heads,\n        dropout_rate=0.0,\n        layernorm_epsilon=1e-6,\n        use_quick_gelu=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n    self.vocab_size = vocab_size\n    self.token_embedding = nnx.Embed(\n        num_embeddings=vocab_size,\n        features=transformer_width,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n    self.ln_final = nnx.LayerNorm(\n        transformer_width,\n        epsilon=1e-6,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    self.text_projection = nnx.Linear(\n        transformer_width,\n        transformer_width,\n        use_bias=True,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n    self.logit_bias = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n</code></pre>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP.encode_image","title":"<code>encode_image(image)</code>","text":"<p>Encode images into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Image embeddings.</p> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode images into embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Image embeddings.\n    \"\"\"\n    return self.vision_model(image)\n</code></pre>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP.encode_text","title":"<code>encode_text(text)</code>","text":"<p>Encode text tokens into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Text embeddings.</p> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode text tokens into embeddings.\n\n    Args:\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Text embeddings.\n    \"\"\"\n    seq_len = text.shape[1]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n    x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding[:seq_len]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n    x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n    pooled_output = x[:, -1, :]\n    x: Float[Array, \"batch transformer_width\"] = self.text_projection(pooled_output)\n    return x\n</code></pre>"},{"location":"models/SigLIP/#jimm.models.siglip.SigLIP.from_pretrained","title":"<code>from_pretrained(model_name_or_path, use_pytorch=False, mesh=None, dtype=jnp.float32)</code>  <code>classmethod</code>","text":"<p>Load a pretrained SigLIP model from a local path or HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path to local weights or HuggingFace model ID.</p> required <code>use_pytorch</code> <code>bool</code> <p>Whether to load from PyTorch weights. Defaults to False.</p> <code>False</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional device mesh for parameter sharding. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>SigLIP</code> <code>SigLIP</code> <p>Pretrained SigLIP model</p> Source code in <code>src/jimm/models/siglip.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"SigLIP\":\n    \"\"\"Load a pretrained SigLIP model from a local path or HuggingFace Hub.\n\n    Args:\n        model_name_or_path (str): Path to local weights or HuggingFace model ID.\n        use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n        mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n    Returns:\n        SigLIP: Pretrained SigLIP model\n    \"\"\"\n    params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n    config: dict[str, Any] = config_dict\n\n    vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[3]\n    vision_width = params_fstate[\"vision_model.embeddings.patch_embedding.bias\"].shape[0]\n    vision_num_layers = 0\n    for k in params_fstate:\n        if k.startswith(\"vision_model.encoder.layers.\") and k.endswith(\".mlp.fc2.bias\"):\n            vision_num_layers = max(vision_num_layers, int(k.split(\".\")[3]) + 1)\n\n    context_length = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n    vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n    text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n    text_num_layers = 0\n    for k_param in params_fstate:\n        if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n            layer_idx = int(k_param.split(\".\")[3])\n            text_num_layers = max(text_num_layers, layer_idx + 1)\n\n    model = cls(\n        image_resolution=config[\"vision_config\"][\"image_size\"],\n        vision_layers=vision_num_layers,\n        vision_width=vision_width,\n        vision_patch_size=vision_patch_size,\n        context_length=context_length,\n        vocab_size=vocab_size,\n        transformer_width=text_hidden_size,\n        transformer_heads=text_hidden_size // 64,\n        transformer_layers=text_num_layers,\n        mesh=mesh,\n        dtype=dtype,\n        param_dtype=dtype,\n    )\n\n    flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n    nonvisited = set(flax_model_params_fstate.keys())\n    used_hf_keys: Set[str] = set()\n\n    mapping_list = [\n        ((\"logit_scale\",), (\"logit_scale\",)),\n        ((\"logit_bias\",), (\"logit_bias\",)),\n        ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n        ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n        ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n        ((\"text_projection\", \"kernel\"), (\"text_model\", \"head\", \"weight\")),\n        ((\"text_projection\", \"bias\"), (\"text_model\", \"head\", \"bias\")),\n        ((\"vision_model\", \"patch_embeddings\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n        ((\"vision_model\", \"patch_embeddings\", \"bias\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"bias\")),\n        ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n        ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n        ((\"vision_model\", \"MAPHead\", \"probe\"), (\"vision_model\", \"head\", \"probe\")),\n        ((\"vision_model\", \"MAPHead\", \"layernorm\", \"scale\"), (\"vision_model\", \"head\", \"layernorm\", \"weight\")),\n        ((\"vision_model\", \"MAPHead\", \"layernorm\", \"bias\"), (\"vision_model\", \"head\", \"layernorm\", \"bias\")),\n        ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 0, \"kernel\"), (\"vision_model\", \"head\", \"mlp\", \"fc1\", \"weight\")),\n        ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 0, \"bias\"), (\"vision_model\", \"head\", \"mlp\", \"fc1\", \"bias\")),\n        ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 2, \"kernel\"), (\"vision_model\", \"head\", \"mlp\", \"fc2\", \"weight\")),\n        ((\"vision_model\", \"MAPHead\", \"mlp\", \"layers\", 2, \"bias\"), (\"vision_model\", \"head\", \"mlp\", \"fc2\", \"bias\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"query\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"query\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"key\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"key\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"value\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_weight\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"value\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"in_proj_bias\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"out\", \"kernel\"), (\"vision_model\", \"head\", \"attention\", \"out_proj\", \"weight\")),\n        ((\"vision_model\", \"MAPHead\", \"attn\", \"out\", \"bias\"), (\"vision_model\", \"head\", \"attention\", \"out_proj\", \"bias\")),\n    ]\n\n    for i in range(text_num_layers):\n        flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n        hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    for i in range(vision_num_layers):\n        flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n        hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    params_name_mapping = dict(mapping_list)\n\n    for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n        hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n        nonvisited.discard(flax_dst_key_tuple)\n        used_hf_keys.add(hf_src_key_as_string)\n        src_value = params_fstate[hf_src_key_as_string]\n        dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n        original_param_sharding = dst_value_obj.value.sharding\n\n        if flax_dst_key_tuple == (\"vision_model\", \"patch_embeddings\", \"kernel\"):\n            src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n        elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n            src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n        elif flax_dst_key_tuple in [(\"logit_scale\",), (\"logit_bias\",)]:\n            src_value = jnp.squeeze(src_value)\n        elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if \"text_model\" in hf_src_key_as_string:\n                num_heads = model.transformer_heads\n                head_dim = model.transformer_width // num_heads\n                src_value = src_value.reshape((model.transformer_width, num_heads, head_dim))\n            else:\n                num_heads = model.vision_heads\n                head_dim = vision_width // num_heads\n                src_value = src_value.reshape((vision_width, num_heads, head_dim))\n        elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            if \"text_model\" in hf_src_key_as_string:\n                num_heads = model.transformer_heads\n                head_dim = model.transformer_width // num_heads\n            else:\n                num_heads = model.vision_heads\n                head_dim = vision_width // num_heads\n            src_value = src_value.reshape((num_heads, head_dim))\n        elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if \"text_model\" in hf_src_key_as_string:\n                num_heads = model.transformer_heads\n                head_dim = model.transformer_width // num_heads\n                src_value = src_value.reshape((num_heads, head_dim, model.transformer_width))\n            else:\n                num_heads = model.vision_heads\n                head_dim = vision_width // num_heads\n                src_value = src_value.reshape((num_heads, head_dim, vision_width))\n        elif hf_src_key_tuple[-1] == \"in_proj_weight\":\n            num_heads = model.vision_heads\n            head_dim = vision_width // num_heads\n            q_w, k_w, v_w = jnp.split(src_value, 3, axis=0)\n            w_map = {\"query\": q_w, \"key\": k_w, \"value\": v_w}\n            src_value = jnp.transpose(w_map[flax_dst_key_tuple[-2]], (1, 0)).reshape(vision_width, num_heads, head_dim)\n        elif hf_src_key_tuple[-1] == \"in_proj_bias\":\n            num_heads = model.vision_heads\n            head_dim = vision_width // num_heads\n            q_b, k_b, v_b = jnp.split(src_value, 3, axis=0)\n            b_map = {\"query\": q_b, \"key\": k_b, \"value\": v_b}\n            src_value = b_map[flax_dst_key_tuple[-2]].reshape(num_heads, head_dim)\n        elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n            if \"position_embedding\" not in hf_src_key_as_string and \"token_embedding\" not in hf_src_key_as_string:\n                src_value = jnp.transpose(src_value, (1, 0))\n        if src_value.shape != dst_value_obj.value.shape:\n            raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n        sharded_new_value = jax.device_put(src_value, original_param_sharding)\n        dst_value_obj.value = sharded_new_value\n\n    nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n    hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n    leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n    known_unused_hf_buffer_keys = {\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    }\n    unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n    assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n    return model\n</code></pre>"},{"location":"models/ViT/","title":"ViT (Vision Transformer)","text":"<p>The ViT (Vision Transformer) is a transformer-based neural network architecture for image classification. It divides an image into fixed-size patches, linearly embeds each patch, adds position embeddings, and processes the resulting sequence of vectors through a standard transformer encoder.</p> <p>The ViT model was introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" and has shown strong performance on image classification benchmarks.</p>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer","title":"<code>jimm.models.vit.VisionTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer (ViT) model for image classification.</p> <p>This implements the Vision Transformer as described in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>class VisionTransformer(nnx.Module):\n    \"\"\"Vision Transformer (ViT) model for image classification.\n\n    This implements the Vision Transformer as described in the paper\n    \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        use_quick_gelu: bool = False,\n        do_classification: bool = True,\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        mesh: Mesh | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a Vision Transformer.\n\n        Args:\n            num_classes (int): Number of output classes. Defaults to 1000.\n            in_channels (int): Number of input channels. Defaults to 3.\n            img_size (int): Size of the input image (assumed square). Defaults to 224.\n            patch_size (int): Size of each patch (assumed square). Defaults to 16.\n            num_layers (int): Number of transformer layers. Defaults to 12.\n            num_heads (int): Number of attention heads. Defaults to 12.\n            mlp_dim (int): Size of the MLP dimension. Defaults to 3072.\n            hidden_size (int): Size of the hidden dimension. Defaults to 768.\n            dropout_rate (float): Dropout rate. Defaults to 0.1.\n            use_quick_gelu (bool): Whether to use quickgelu instead of gelu. Defaults to False.\n            do_classification (bool): Whether to include the final classification head. Defaults to True.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): Data type for parameters. Defaults to jnp.float32.\n            rngs (nnx.Rngs): Random number generator keys. Defaults to nnx.Rngs(0).\n            mesh (Mesh|None): Optional JAX device mesh for parameter sharding. Defaults to None.\n        \"\"\"\n        self.do_classification = do_classification\n        self.encoder = VisionTransformerBase(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_channels=in_channels,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            num_heads=num_heads,\n            mlp_dim=mlp_dim,\n            dropout_rate=dropout_rate,\n            use_quick_gelu=use_quick_gelu,\n            use_pre_norm=False,\n            use_patch_bias=True,\n            layernorm_epsilon=1e-12,\n            rngs=rngs,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n        )\n\n        if self.do_classification:\n            self.classifier = nnx.Linear(\n                hidden_size,\n                num_classes,\n                dtype=dtype,\n                param_dtype=param_dtype,\n                rngs=rngs,\n                kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n                bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n            )\n\n    def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch num_classes\"]:\n        \"\"\"Forward pass of the Vision Transformer.\n\n        Args:\n            x (Float[Array, \"batch height width channels\"]): Input tensor with shape [batch, height, width, channels]\n\n        Returns:\n            Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]\n        \"\"\"\n        x = self.encoder(x)\n        if self.do_classification:\n            return self.classifier(x)\n        return x\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"VisionTransformer\":\n        \"\"\"Load a pretrained Vision Transformer from a local path or HuggingFace Hub.\n\n        Args:\n            model_name_or_path (str): Path to local weights or HuggingFace model ID.\n            use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n            mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n        Returns:\n            VisionTransformer: Initialized Vision Transformer with pretrained weights\n        \"\"\"\n        params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n        config: dict[str, Any] = config_dict\n\n        hidden_size_val: int\n        num_classes_val: int\n        num_layers_val: int\n        num_heads_val: int\n        mlp_dim_val: int\n        patch_size_val: int\n        img_size_val: int\n        use_quick_gelu_val: bool = False\n\n        if config:\n            hidden_size_val = config[\"hidden_size\"]\n            num_classes_val = len(config[\"id2label\"]) if \"id2label\" in config else config.get(\"num_labels\", 1000)\n            num_layers_val = config[\"num_hidden_layers\"]\n            num_heads_val = config[\"num_attention_heads\"]\n            mlp_dim_val = config[\"intermediate_size\"]\n            patch_size_val = config[\"patch_size\"]\n            img_size_val = config[\"image_size\"]\n            if \"hidden_act\" in config and config[\"hidden_act\"] == \"quick_gelu\":\n                use_quick_gelu_val = True\n            elif \"hidden_act\" in config and config[\"hidden_act\"] != \"gelu\":\n                print(f\"Warning: Unexpected hidden_act '{config['hidden_act']}' in config, defaulting to standard GELU.\")\n\n        elif not use_pytorch and (os.path.exists(model_name_or_path) and os.path.isfile(model_name_or_path)):\n            hidden_size_val = params_fstate[\"vit.embeddings.cls_token\"].shape[-1]\n            num_classes_val = params_fstate[\"classifier.bias\"].shape[0]\n\n            max_layer_idx = -1\n            for k in params_fstate:\n                if k.startswith(\"vit.encoder.layer.\"):\n                    max_layer_idx = max(max_layer_idx, int(k.split(\".\")[3]))\n            num_layers_val = max_layer_idx + 1\n\n            mlp_dim_val = params_fstate[\"vit.encoder.layer.0.intermediate.dense.weight\"].shape[0]\n\n            assumed_head_dim = 64\n            num_heads_val = hidden_size_val // assumed_head_dim\n\n            patch_kernel_shape = params_fstate[\"vit.embeddings.patch_embeddings.projection.weight\"].shape\n            patch_size_val = patch_kernel_shape[2]\n\n            num_patches_from_embeddings = params_fstate[\"vit.embeddings.position_embeddings\"].shape[1] - 1\n            img_size_dim = int(jnp.sqrt(num_patches_from_embeddings))\n            img_size_val = img_size_dim * patch_size_val\n        else:\n            raise ValueError(f\"Could not load or infer configuration for {model_name_or_path}\")\n\n        if not all(v is not None for v in [hidden_size_val, num_classes_val, num_layers_val, num_heads_val, mlp_dim_val, patch_size_val, img_size_val]):\n            raise ValueError(f\"One or more configuration parameters could not be determined for {model_name_or_path}\")\n\n        model = cls(\n            num_classes=num_classes_val,\n            img_size=img_size_val,\n            patch_size=patch_size_val,\n            num_layers=num_layers_val,\n            num_heads=num_heads_val,\n            mlp_dim=mlp_dim_val,\n            hidden_size=hidden_size_val,\n            use_quick_gelu=use_quick_gelu_val,\n            mesh=mesh,\n            dtype=dtype,\n            param_dtype=dtype,\n        )\n\n        flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n        def hf_param_name(name: str) -&gt; str:\n            return \"weight\" if name in [\"kernel\", \"scale\"] else name\n\n        hidden_size_per_head = hidden_size_val // num_heads_val\n\n        mapping_list = [\n            ((\"encoder\", \"cls_token\"), (\"vit\", \"embeddings\", \"cls_token\")),\n            ((\"encoder\", \"position_embeddings\"), (\"vit\", \"embeddings\", \"position_embeddings\")),\n            ((\"encoder\", \"patch_embeddings\", \"kernel\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"weight\")),\n            ((\"encoder\", \"patch_embeddings\", \"bias\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"bias\")),\n            ((\"classifier\", \"kernel\"), (\"classifier\", \"weight\")),\n            ((\"classifier\", \"bias\"), (\"classifier\", \"bias\")),\n            ((\"encoder\", \"ln_post\", \"scale\"), (\"vit\", \"layernorm\", \"weight\")),\n            ((\"encoder\", \"ln_post\", \"bias\"), (\"vit\", \"layernorm\", \"bias\")),\n        ]\n\n        for i in range(num_layers_val):\n            flax_base = (\"encoder\", \"transformer\", \"blocks\", \"layers\", i)\n            hf_base = (\"vit\", \"encoder\", \"layer\", str(i))\n            mapping_list.extend(\n                [(flax_base + (\"attn\", y_type, p_name), hf_base + (\"attention\", \"attention\", y_type, hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"] for y_type in [\"key\", \"value\", \"query\"]]\n            )\n            mapping_list.extend([(flax_base + (\"attn\", \"out\", p_name), hf_base + (\"attention\", \"output\", \"dense\", hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"]])\n            mapping_list.extend(\n                [\n                    (flax_base + (\"mlp\", \"layers\", y1_idx, p_name), hf_base + (y2_name, \"dense\", hf_param_name(p_name)))\n                    for p_name in [\"kernel\", \"bias\"]\n                    for y1_idx, y2_name in [(0, \"intermediate\"), (3, \"output\")]\n                ]\n            )\n            mapping_list.extend(\n                [\n                    (flax_base + (norm_flax, p_name), hf_base + (norm_hf, hf_param_name(p_name)))\n                    for p_name in [\"scale\", \"bias\"]\n                    for norm_flax, norm_hf in [(\"norm1\", \"layernorm_before\"), (\"norm2\", \"layernorm_after\")]\n                ]\n            )\n        params_name_mapping = dict(mapping_list)\n        nonvisited = set(flax_model_params_fstate.keys())\n        used_hf_keys: Set[str] = set()\n\n        for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n            assert flax_dst_key_tuple in flax_model_params_fstate, flax_dst_key_tuple\n            hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n            used_hf_keys.add(hf_src_key_as_string)\n            assert hf_src_key_as_string in params_fstate, f\"HF key '{hf_src_key_as_string}' (from Flax key {flax_dst_key_tuple}) not found in loaded weights.\"\n            nonvisited.remove(flax_dst_key_tuple)\n            src_value: Array = params_fstate[hf_src_key_as_string]\n\n            dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n            original_param_sharding = dst_value_obj.value.sharding\n\n            if flax_dst_key_tuple == (\"encoder\", \"patch_embeddings\", \"kernel\"):\n                src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n            elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                src_value = src_value.reshape((hidden_size_val, num_heads_val, hidden_size_per_head))\n            elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n                src_value = src_value.reshape((num_heads_val, hidden_size_per_head))\n            elif hf_src_key_tuple[-4:] == (\"attention\", \"output\", \"dense\", \"weight\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                src_value = src_value.reshape((num_heads_val, hidden_size_per_head, hidden_size_val))\n            elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n                src_value = jnp.transpose(src_value, (1, 0))\n\n            assert src_value.shape == dst_value_obj.value.shape, f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} != {src_value.shape}\"\n\n            sharded_new_value: Array = jax.device_put(src_value, original_param_sharding)\n            dst_value_obj.value = sharded_new_value\n\n            assert jnp.allclose(dst_value_obj.value.mean(), src_value.mean()), (dst_value_obj.value.mean(), src_value.mean())\n\n        assert len(nonvisited) == 0, f\"Some Flax model parameters were not visited: {nonvisited}\"\n\n        leftover_hf_keys = set(params_fstate.keys()) - used_hf_keys\n        known_unused_hf_buffer_keys = {\n            \"text_model.embeddings.position_ids\",\n            \"vision_model.embeddings.position_ids\",\n        }\n        unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n        assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n        nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n        del flax_model_params_fstate\n        del params_fstate\n        return model\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward pass of the Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'batch height width channels']</code> <p>Input tensor with shape [batch, height, width, channels]</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch num_classes']</code> <p>Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch num_classes\"]:\n    \"\"\"Forward pass of the Vision Transformer.\n\n    Args:\n        x (Float[Array, \"batch height width channels\"]): Input tensor with shape [batch, height, width, channels]\n\n    Returns:\n        Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]\n    \"\"\"\n    x = self.encoder(x)\n    if self.do_classification:\n        return self.classifier(x)\n    return x\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.__init__","title":"<code>__init__(num_classes=1000, in_channels=3, img_size=224, patch_size=16, num_layers=12, num_heads=12, mlp_dim=3072, hidden_size=768, dropout_rate=0.1, use_quick_gelu=False, do_classification=True, dtype=jnp.float32, param_dtype=jnp.float32, rngs=nnx.Rngs(0), mesh=None)</code>","text":"<p>Initialize a Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes. Defaults to 1000.</p> <code>1000</code> <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>img_size</code> <code>int</code> <p>Size of the input image (assumed square). Defaults to 224.</p> <code>224</code> <code>patch_size</code> <code>int</code> <p>Size of each patch (assumed square). Defaults to 16.</p> <code>16</code> <code>num_layers</code> <code>int</code> <p>Number of transformer layers. Defaults to 12.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads. Defaults to 12.</p> <code>12</code> <code>mlp_dim</code> <code>int</code> <p>Size of the MLP dimension. Defaults to 3072.</p> <code>3072</code> <code>hidden_size</code> <code>int</code> <p>Size of the hidden dimension. Defaults to 768.</p> <code>768</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate. Defaults to 0.1.</p> <code>0.1</code> <code>use_quick_gelu</code> <code>bool</code> <p>Whether to use quickgelu instead of gelu. Defaults to False.</p> <code>False</code> <code>do_classification</code> <code>bool</code> <p>Whether to include the final classification head. Defaults to True.</p> <code>True</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>Data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>rngs</code> <code>Rngs</code> <p>Random number generator keys. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional JAX device mesh for parameter sharding. Defaults to None.</p> <code>None</code> Source code in <code>src/jimm/models/vit.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    img_size: int = 224,\n    patch_size: int = 16,\n    num_layers: int = 12,\n    num_heads: int = 12,\n    mlp_dim: int = 3072,\n    hidden_size: int = 768,\n    dropout_rate: float = 0.1,\n    use_quick_gelu: bool = False,\n    do_classification: bool = True,\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    mesh: Mesh | None = None,\n) -&gt; None:\n    \"\"\"Initialize a Vision Transformer.\n\n    Args:\n        num_classes (int): Number of output classes. Defaults to 1000.\n        in_channels (int): Number of input channels. Defaults to 3.\n        img_size (int): Size of the input image (assumed square). Defaults to 224.\n        patch_size (int): Size of each patch (assumed square). Defaults to 16.\n        num_layers (int): Number of transformer layers. Defaults to 12.\n        num_heads (int): Number of attention heads. Defaults to 12.\n        mlp_dim (int): Size of the MLP dimension. Defaults to 3072.\n        hidden_size (int): Size of the hidden dimension. Defaults to 768.\n        dropout_rate (float): Dropout rate. Defaults to 0.1.\n        use_quick_gelu (bool): Whether to use quickgelu instead of gelu. Defaults to False.\n        do_classification (bool): Whether to include the final classification head. Defaults to True.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): Data type for parameters. Defaults to jnp.float32.\n        rngs (nnx.Rngs): Random number generator keys. Defaults to nnx.Rngs(0).\n        mesh (Mesh|None): Optional JAX device mesh for parameter sharding. Defaults to None.\n    \"\"\"\n    self.do_classification = do_classification\n    self.encoder = VisionTransformerBase(\n        img_size=img_size,\n        patch_size=patch_size,\n        in_channels=in_channels,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        num_heads=num_heads,\n        mlp_dim=mlp_dim,\n        dropout_rate=dropout_rate,\n        use_quick_gelu=use_quick_gelu,\n        use_pre_norm=False,\n        use_patch_bias=True,\n        layernorm_epsilon=1e-12,\n        rngs=rngs,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n    )\n\n    if self.do_classification:\n        self.classifier = nnx.Linear(\n            hidden_size,\n            num_classes,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.from_pretrained","title":"<code>from_pretrained(model_name_or_path, use_pytorch=False, mesh=None, dtype=jnp.float32)</code>  <code>classmethod</code>","text":"<p>Load a pretrained Vision Transformer from a local path or HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path to local weights or HuggingFace model ID.</p> required <code>use_pytorch</code> <code>bool</code> <p>Whether to load from PyTorch weights. Defaults to False.</p> <code>False</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional device mesh for parameter sharding. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>VisionTransformer</code> <code>VisionTransformer</code> <p>Initialized Vision Transformer with pretrained weights</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"VisionTransformer\":\n    \"\"\"Load a pretrained Vision Transformer from a local path or HuggingFace Hub.\n\n    Args:\n        model_name_or_path (str): Path to local weights or HuggingFace model ID.\n        use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n        mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n    Returns:\n        VisionTransformer: Initialized Vision Transformer with pretrained weights\n    \"\"\"\n    params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n    config: dict[str, Any] = config_dict\n\n    hidden_size_val: int\n    num_classes_val: int\n    num_layers_val: int\n    num_heads_val: int\n    mlp_dim_val: int\n    patch_size_val: int\n    img_size_val: int\n    use_quick_gelu_val: bool = False\n\n    if config:\n        hidden_size_val = config[\"hidden_size\"]\n        num_classes_val = len(config[\"id2label\"]) if \"id2label\" in config else config.get(\"num_labels\", 1000)\n        num_layers_val = config[\"num_hidden_layers\"]\n        num_heads_val = config[\"num_attention_heads\"]\n        mlp_dim_val = config[\"intermediate_size\"]\n        patch_size_val = config[\"patch_size\"]\n        img_size_val = config[\"image_size\"]\n        if \"hidden_act\" in config and config[\"hidden_act\"] == \"quick_gelu\":\n            use_quick_gelu_val = True\n        elif \"hidden_act\" in config and config[\"hidden_act\"] != \"gelu\":\n            print(f\"Warning: Unexpected hidden_act '{config['hidden_act']}' in config, defaulting to standard GELU.\")\n\n    elif not use_pytorch and (os.path.exists(model_name_or_path) and os.path.isfile(model_name_or_path)):\n        hidden_size_val = params_fstate[\"vit.embeddings.cls_token\"].shape[-1]\n        num_classes_val = params_fstate[\"classifier.bias\"].shape[0]\n\n        max_layer_idx = -1\n        for k in params_fstate:\n            if k.startswith(\"vit.encoder.layer.\"):\n                max_layer_idx = max(max_layer_idx, int(k.split(\".\")[3]))\n        num_layers_val = max_layer_idx + 1\n\n        mlp_dim_val = params_fstate[\"vit.encoder.layer.0.intermediate.dense.weight\"].shape[0]\n\n        assumed_head_dim = 64\n        num_heads_val = hidden_size_val // assumed_head_dim\n\n        patch_kernel_shape = params_fstate[\"vit.embeddings.patch_embeddings.projection.weight\"].shape\n        patch_size_val = patch_kernel_shape[2]\n\n        num_patches_from_embeddings = params_fstate[\"vit.embeddings.position_embeddings\"].shape[1] - 1\n        img_size_dim = int(jnp.sqrt(num_patches_from_embeddings))\n        img_size_val = img_size_dim * patch_size_val\n    else:\n        raise ValueError(f\"Could not load or infer configuration for {model_name_or_path}\")\n\n    if not all(v is not None for v in [hidden_size_val, num_classes_val, num_layers_val, num_heads_val, mlp_dim_val, patch_size_val, img_size_val]):\n        raise ValueError(f\"One or more configuration parameters could not be determined for {model_name_or_path}\")\n\n    model = cls(\n        num_classes=num_classes_val,\n        img_size=img_size_val,\n        patch_size=patch_size_val,\n        num_layers=num_layers_val,\n        num_heads=num_heads_val,\n        mlp_dim=mlp_dim_val,\n        hidden_size=hidden_size_val,\n        use_quick_gelu=use_quick_gelu_val,\n        mesh=mesh,\n        dtype=dtype,\n        param_dtype=dtype,\n    )\n\n    flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n    def hf_param_name(name: str) -&gt; str:\n        return \"weight\" if name in [\"kernel\", \"scale\"] else name\n\n    hidden_size_per_head = hidden_size_val // num_heads_val\n\n    mapping_list = [\n        ((\"encoder\", \"cls_token\"), (\"vit\", \"embeddings\", \"cls_token\")),\n        ((\"encoder\", \"position_embeddings\"), (\"vit\", \"embeddings\", \"position_embeddings\")),\n        ((\"encoder\", \"patch_embeddings\", \"kernel\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"weight\")),\n        ((\"encoder\", \"patch_embeddings\", \"bias\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"bias\")),\n        ((\"classifier\", \"kernel\"), (\"classifier\", \"weight\")),\n        ((\"classifier\", \"bias\"), (\"classifier\", \"bias\")),\n        ((\"encoder\", \"ln_post\", \"scale\"), (\"vit\", \"layernorm\", \"weight\")),\n        ((\"encoder\", \"ln_post\", \"bias\"), (\"vit\", \"layernorm\", \"bias\")),\n    ]\n\n    for i in range(num_layers_val):\n        flax_base = (\"encoder\", \"transformer\", \"blocks\", \"layers\", i)\n        hf_base = (\"vit\", \"encoder\", \"layer\", str(i))\n        mapping_list.extend(\n            [(flax_base + (\"attn\", y_type, p_name), hf_base + (\"attention\", \"attention\", y_type, hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"] for y_type in [\"key\", \"value\", \"query\"]]\n        )\n        mapping_list.extend([(flax_base + (\"attn\", \"out\", p_name), hf_base + (\"attention\", \"output\", \"dense\", hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"]])\n        mapping_list.extend(\n            [\n                (flax_base + (\"mlp\", \"layers\", y1_idx, p_name), hf_base + (y2_name, \"dense\", hf_param_name(p_name)))\n                for p_name in [\"kernel\", \"bias\"]\n                for y1_idx, y2_name in [(0, \"intermediate\"), (3, \"output\")]\n            ]\n        )\n        mapping_list.extend(\n            [\n                (flax_base + (norm_flax, p_name), hf_base + (norm_hf, hf_param_name(p_name)))\n                for p_name in [\"scale\", \"bias\"]\n                for norm_flax, norm_hf in [(\"norm1\", \"layernorm_before\"), (\"norm2\", \"layernorm_after\")]\n            ]\n        )\n    params_name_mapping = dict(mapping_list)\n    nonvisited = set(flax_model_params_fstate.keys())\n    used_hf_keys: Set[str] = set()\n\n    for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n        assert flax_dst_key_tuple in flax_model_params_fstate, flax_dst_key_tuple\n        hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n        used_hf_keys.add(hf_src_key_as_string)\n        assert hf_src_key_as_string in params_fstate, f\"HF key '{hf_src_key_as_string}' (from Flax key {flax_dst_key_tuple}) not found in loaded weights.\"\n        nonvisited.remove(flax_dst_key_tuple)\n        src_value: Array = params_fstate[hf_src_key_as_string]\n\n        dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n        original_param_sharding = dst_value_obj.value.sharding\n\n        if flax_dst_key_tuple == (\"encoder\", \"patch_embeddings\", \"kernel\"):\n            src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n        elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            src_value = src_value.reshape((hidden_size_val, num_heads_val, hidden_size_per_head))\n        elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n            src_value = src_value.reshape((num_heads_val, hidden_size_per_head))\n        elif hf_src_key_tuple[-4:] == (\"attention\", \"output\", \"dense\", \"weight\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            src_value = src_value.reshape((num_heads_val, hidden_size_per_head, hidden_size_val))\n        elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n            src_value = jnp.transpose(src_value, (1, 0))\n\n        assert src_value.shape == dst_value_obj.value.shape, f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} != {src_value.shape}\"\n\n        sharded_new_value: Array = jax.device_put(src_value, original_param_sharding)\n        dst_value_obj.value = sharded_new_value\n\n        assert jnp.allclose(dst_value_obj.value.mean(), src_value.mean()), (dst_value_obj.value.mean(), src_value.mean())\n\n    assert len(nonvisited) == 0, f\"Some Flax model parameters were not visited: {nonvisited}\"\n\n    leftover_hf_keys = set(params_fstate.keys()) - used_hf_keys\n    known_unused_hf_buffer_keys = {\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    }\n    unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n    assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n    nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n    del flax_model_params_fstate\n    del params_fstate\n    return model\n</code></pre>"}]}